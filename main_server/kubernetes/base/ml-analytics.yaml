apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-analytics-config
  namespace: rabere-habitat
data:
  config.yaml: |
    service:
      name: ml-analytics
      environment: production
      log_level: INFO
    
    model:
      version: "1.0.0"
      batch_size: 32
      inference_timeout: 30
      max_queue_size: 100
    
    monitoring:
      metrics_port: 9090
      profiler_enabled: true
      trace_sampling_rate: 0.1
    
    gpu:
      memory_fraction: 0.8
      allow_growth: true
      per_process_gpu_memory_fraction: 0.7
---
apiVersion: v1
kind: Service
metadata:
  name: ml-analytics
  namespace: rabere-habitat
  labels:
    app: ml-analytics
    component: inference
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  type: ClusterIP
  ports:
    - port: 8501
      targetPort: tensorflow
      protocol: TCP
      name: tensorflow
    - port: 9090
      targetPort: metrics
      protocol: TCP
      name: metrics
  selector:
    app: ml-analytics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-analytics
  namespace: rabere-habitat
  labels:
    app: ml-analytics
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ml-analytics
  template:
    metadata:
      labels:
        app: ml-analytics
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: ml-analytics-sa
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: ml-analytics
          image: ahecs/ml-analytics:latest
          imagePullPolicy: Always
          resources:
            limits:
              nvidia.com/gpu: 1
              cpu: "4"
              memory: "8Gi"
            requests:
              cpu: "2"
              memory: "4Gi"
          ports:
            - name: tensorflow
              containerPort: 8501
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: MODEL_PATH
              value: "/models"
            - name: TENSORFLOW_SERVING_PORT
              value: "8501"
            - name: METRICS_PORT
              value: "9090"
          volumeMounts:
            - name: models
              mountPath: /models
              readOnly: true
            - name: config
              mountPath: /app/config
              readOnly: true
            - name: cache
              mountPath: /app/cache
          livenessProbe:
            httpGet:
              path: /health
              port: metrics
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /ready
              port: metrics
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /startup
              port: metrics
            failureThreshold: 30
            periodSeconds: 10
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ml-models-pvc
        - name: config
          configMap:
            name: ml-analytics-config
        - name: cache
          emptyDir: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: nvidia.com/gpu
                    operator: Exists
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - ml-analytics
                topologyKey: kubernetes.io/hostname
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ml-analytics-pdb
  namespace: rabere-habitat
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: ml-analytics
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-analytics
  namespace: rabere-habitat
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-analytics
  minReplicas: 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ml-analytics-network-policy
  namespace: rabere-habitat
spec:
  podSelector:
    matchLabels:
      app: ml-analytics
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: api-gateway
        - podSelector:
            matchLabels:
              app: telemetry-service
      ports:
        - protocol: TCP
          port: 8501
        - protocol: TCP
          port: 9090
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379
    - to:
        - podSelector:
            matchLabels:
              app: mongodb
      ports:
        - protocol: TCP
          port: 27017 